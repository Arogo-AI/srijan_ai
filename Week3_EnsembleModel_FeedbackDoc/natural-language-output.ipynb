{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10722068,"sourceType":"datasetVersion","datasetId":6646532},{"sourceId":10725968,"sourceType":"datasetVersion","datasetId":6649440},{"sourceId":10726089,"sourceType":"datasetVersion","datasetId":6649528}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfinal_df = pd.read_csv('/kaggle/input/df-final/combined_cleaned_df_final.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T19:45:47.864450Z","iopub.execute_input":"2025-02-11T19:45:47.864705Z","iopub.status.idle":"2025-02-11T19:45:49.980797Z","shell.execute_reply.started":"2025-02-11T19:45:47.864669Z","shell.execute_reply":"2025-02-11T19:45:49.980068Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"final_df = final_df.drop('Unnamed: 0', axis=1)\nfrom sklearn.preprocessing import LabelEncoder\n\ngender_encoder = LabelEncoder()\n\nfinal_df['gender'] = gender_encoder.fit_transform(final_df['gender'])\ndf_icd9 = final_df[final_df['icd_version'] == 9]\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nicd_encoded = encoder.fit_transform(df_icd9[['icd_code']])\nicd_encoded_df = pd.DataFrame(\n    icd_encoded,\n    columns=encoder.get_feature_names_out(['icd_code'])\n)\nX = df_icd9.drop(['hadm_id', 'icd_code', 'subject_id'], axis=1)\ny = icd_encoded_df\ny_labels = y.idxmax(axis=1)\nX.columns = X.columns.astype(str)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() \nX_scaled = scaler.fit_transform(X)\n\nimport numpy as np\ny_class = np.argmax(y.values, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T19:45:54.332940Z","iopub.execute_input":"2025-02-11T19:45:54.333284Z","iopub.status.idle":"2025-02-11T19:45:55.004704Z","shell.execute_reply.started":"2025-02-11T19:45:54.333256Z","shell.execute_reply":"2025-02-11T19:45:55.003897Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport lightgbm as lgb\nimport joblib\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------------\n# 1. Define the PyTorch model architecture\n# ------------------------------------------------------------------------------\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size=25, hidden1=256, hidden2=512, hidden3=256, hidden4=128, num_classes=1203):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden1)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden2, hidden3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden3, hidden4)\n        self.relu4 = nn.ReLU()\n        self.fc5 = nn.Linear(hidden4, num_classes)\n    \n    def forward(self, x):\n        x = self.relu1(self.fc1(x))\n        x = self.relu2(self.fc2(x))\n        x = self.relu3(self.fc3(x))\n        x = self.relu4(self.fc4(x))\n        x = self.fc5(x)  # Raw logits (will apply softmax later)\n        return x\n\n# ------------------------------------------------------------------------------\n# 2. Load the pre-trained models\n# ------------------------------------------------------------------------------\n\n# Load LightGBM model (assumed saved as a text file)\nlgb_model_path = '/kaggle/input/models-arogo/lgb_model.txt'  # Update with your actual file path\nloaded_lgb = lgb.Booster(model_file=lgb_model_path)\n\n# Load XGBoost model from a pickle file\nxgb_model_path = '/kaggle/input/models-arogo/xgb_model.pkl'  # Update with your actual file path\nloaded_xgb = joblib.load(xgb_model_path)\n\n# Load PyTorch model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npytorch_model_path = '/kaggle/input/models-arogo/simple_net_state_dict.pth'  # Update with your actual file path\npytorch_model = SimpleNet(input_size=25, num_classes=1203).to(device)\npytorch_model.load_state_dict(torch.load(pytorch_model_path, map_location=device))\npytorch_model.eval()\n\n# ------------------------------------------------------------------------------\n# 3. Define the ensemble prediction functions\n# ------------------------------------------------------------------------------\n\ndef ensemble_predict(X, weights=None):\n    \"\"\"\n    Returns the ensemble's top-1 prediction for each sample.\n    This is useful for computing overall accuracy.\n    \n    Args:\n        X (np.array): Input feature array of shape (n_samples, n_features)\n        weights (list or tuple): Weights for the three models [w_lgb, w_xgb, w_pt].\n                                 Defaults to equal weighting if None.\n    \n    Returns:\n        final_preds (np.array): Array of shape (n_samples,) with the predicted class labels.\n    \"\"\"\n    if weights is None:\n        weights = [1/3, 1/3, 1/3]\n    w_lgb, w_xgb, w_pt = weights\n\n    # LightGBM: Get probability distribution\n    lgb_probs = loaded_lgb.predict(X)  # Shape: (n_samples, num_classes)\n    \n    # XGBoost: Get probability distribution\n    xgb_probs = loaded_xgb.predict_proba(X)  # Shape: (n_samples, num_classes)\n    \n    # PyTorch: Get probability distribution (apply softmax to logits)\n    X_tensor = torch.from_numpy(X).float().to(device)\n    with torch.no_grad():\n        outputs = pytorch_model(X_tensor)\n        pt_probs = F.softmax(outputs, dim=1).cpu().numpy()  # Shape: (n_samples, num_classes)\n    \n    # Compute weighted sum of probabilities\n    ensemble_probs = w_lgb * lgb_probs + w_xgb * xgb_probs + w_pt * pt_probs\n\n    # Return the class with the highest probability for each sample\n    final_preds = np.argmax(ensemble_probs, axis=1)\n    return final_preds\n\ndef ensemble_topk(X, top_k=3, weights=None):\n    \"\"\"\n    Returns the top-k predictions (class indices and their probabilities) for each sample.\n    \n    Args:\n        X (np.array): Input feature array of shape (n_samples, n_features)\n        top_k (int): Number of top predictions to return.\n        weights (list or tuple): Weights for the three models [w_lgb, w_xgb, w_pt].\n                                 Defaults to equal weighting if None.\n    \n    Returns:\n        top_k_indices (np.array): Array of shape (n_samples, top_k) with top-k class indices.\n        top_k_probs (np.array): Array of shape (n_samples, top_k) with corresponding probabilities.\n    \"\"\"\n    if weights is None:\n        weights = [1/3, 1/3, 1/3]\n    w_lgb, w_xgb, w_pt = weights\n\n    # Get probability distributions from each model\n    lgb_probs = loaded_lgb.predict(X)\n    xgb_probs = loaded_xgb.predict_proba(X)\n    X_tensor = torch.from_numpy(X).float().to(device)\n    with torch.no_grad():\n        outputs = pytorch_model(X_tensor)\n        pt_probs = F.softmax(outputs, dim=1).cpu().numpy()\n\n    # Compute weighted sum of probabilities\n    ensemble_probs = w_lgb * lgb_probs + w_xgb * xgb_probs + w_pt * pt_probs\n\n    # For each sample, retrieve the indices of the top k probabilities.\n    top_k_indices = np.argsort(ensemble_probs, axis=1)[:, -top_k:][:, ::-1]\n    top_k_probs = np.take_along_axis(ensemble_probs, top_k_indices, axis=1)\n    \n    return top_k_indices, top_k_probs\n\n# ------------------------------------------------------------------------------\n# 4. Example usage\n# ------------------------------------------------------------------------------\n\n# Assuming X_scaled is your NumPy array of input features:\n# For instance:\n# X_scaled = np.load('X_scaled.npy')\n\n# # Get top-1 predictions (for accuracy computation)\n# final_predictions = ensemble_predict(X_scaled, weights=[0.4, 0.3, 0.3])\n# print(\"Final predictions (top-1) for each sample:\\n\", final_predictions)\n\n# # # Get top-3 predictions (with probabilities)\n# top_k_indices, top_k_probs = ensemble_topk(X_scaled, top_k=3, weights=[0.4, 0.3, 0.3])\n# print(\"Top-3 class indices for each sample:\\n\", top_k_indices)\n# print(\"Corresponding probabilities for top-3 predictions:\\n\", top_k_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:00:10.054334Z","iopub.execute_input":"2025-02-11T20:00:10.054682Z","iopub.status.idle":"2025-02-11T20:00:40.438985Z","shell.execute_reply.started":"2025-02-11T20:00:10.054658Z","shell.execute_reply":"2025-02-11T20:00:40.438051Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-3884dbc84269>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pytorch_model.load_state_dict(torch.load(pytorch_model_path, map_location=device))\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Get top-1 predictions (for accuracy computation)\nfinal_predictions = ensemble_predict(X_scaled, weights=[0.4, 0.3, 0.3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:05:26.531585Z","iopub.execute_input":"2025-02-11T20:05:26.531893Z","iopub.status.idle":"2025-02-11T20:09:17.278726Z","shell.execute_reply.started":"2025-02-11T20:05:26.531866Z","shell.execute_reply":"2025-02-11T20:09:17.277672Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_class, final_predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:10:04.703857Z","iopub.execute_input":"2025-02-11T20:10:04.704220Z","iopub.status.idle":"2025-02-11T20:10:04.710522Z","shell.execute_reply.started":"2025-02-11T20:10:04.704189Z","shell.execute_reply":"2025-02-11T20:10:04.709748Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.98\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tests = list(final_df.columns[5:])\ntests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:24:27.988613Z","iopub.execute_input":"2025-02-11T20:24:27.988896Z","iopub.status.idle":"2025-02-11T20:24:27.994298Z","shell.execute_reply.started":"2025-02-11T20:24:27.988875Z","shell.execute_reply":"2025-02-11T20:24:27.993529Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['50868',\n '50882',\n '50893',\n '50902',\n '50912',\n '50931',\n '50960',\n '50970',\n '50971',\n '50983',\n '51006',\n '51221',\n '51222',\n '51237',\n '51248',\n '51249',\n '51250',\n '51265',\n '51274',\n '51275',\n '51277',\n '51279',\n '51301']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:24:39.234898Z","iopub.execute_input":"2025-02-11T20:24:39.235197Z","iopub.status.idle":"2025-02-11T20:24:39.267895Z","shell.execute_reply.started":"2025-02-11T20:24:39.235154Z","shell.execute_reply":"2025-02-11T20:24:39.267215Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      icd_code_00845  icd_code_0088  icd_code_0090  icd_code_01300  \\\n0                0.0            0.0            0.0             0.0   \n1                0.0            0.0            0.0             0.0   \n2                0.0            0.0            0.0             0.0   \n3                0.0            0.0            0.0             0.0   \n4                0.0            0.0            0.0             0.0   \n...              ...            ...            ...             ...   \n4570             0.0            0.0            0.0             0.0   \n4571             0.0            0.0            0.0             0.0   \n4572             0.0            0.0            0.0             0.0   \n4573             0.0            0.0            0.0             0.0   \n4574             0.0            0.0            0.0             0.0   \n\n      icd_code_01896  icd_code_0340  icd_code_035  icd_code_0380  \\\n0                0.0            0.0           0.0            0.0   \n1                0.0            0.0           0.0            0.0   \n2                0.0            0.0           0.0            0.0   \n3                0.0            0.0           0.0            0.0   \n4                0.0            0.0           0.0            0.0   \n...              ...            ...           ...            ...   \n4570             0.0            0.0           0.0            0.0   \n4571             0.0            0.0           0.0            0.0   \n4572             0.0            0.0           0.0            0.0   \n4573             0.0            0.0           0.0            0.0   \n4574             0.0            0.0           0.0            0.0   \n\n      icd_code_03811  icd_code_03812  ...  icd_code_V5811  icd_code_V5812  \\\n0                0.0             0.0  ...             0.0             0.0   \n1                0.0             0.0  ...             0.0             0.0   \n2                0.0             0.0  ...             0.0             0.0   \n3                0.0             0.0  ...             0.0             0.0   \n4                0.0             0.0  ...             0.0             0.0   \n...              ...             ...  ...             ...             ...   \n4570             0.0             0.0  ...             0.0             0.0   \n4571             0.0             0.0  ...             0.0             0.0   \n4572             0.0             0.0  ...             0.0             0.0   \n4573             0.0             0.0  ...             0.0             0.0   \n4574             0.0             0.0  ...             0.0             0.0   \n\n      icd_code_V5883  icd_code_V600  icd_code_V618  icd_code_V6284  \\\n0                0.0            0.0            0.0             0.0   \n1                0.0            0.0            0.0             0.0   \n2                0.0            0.0            0.0             0.0   \n3                0.0            0.0            0.0             0.0   \n4                0.0            0.0            0.0             0.0   \n...              ...            ...            ...             ...   \n4570             0.0            0.0            0.0             0.0   \n4571             0.0            0.0            0.0             0.0   \n4572             0.0            0.0            0.0             0.0   \n4573             0.0            0.0            0.0             0.0   \n4574             0.0            0.0            0.0             0.0   \n\n      icd_code_V714  icd_code_V7189  icd_code_V7281  icd_code_V7651  \n0               0.0             0.0             0.0             0.0  \n1               0.0             0.0             0.0             0.0  \n2               0.0             0.0             0.0             0.0  \n3               0.0             0.0             0.0             0.0  \n4               0.0             0.0             0.0             0.0  \n...             ...             ...             ...             ...  \n4570            0.0             0.0             0.0             0.0  \n4571            0.0             0.0             0.0             0.0  \n4572            0.0             0.0             0.0             0.0  \n4573            0.0             0.0             0.0             0.0  \n4574            0.0             0.0             0.0             0.0  \n\n[4575 rows x 1203 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>icd_code_00845</th>\n      <th>icd_code_0088</th>\n      <th>icd_code_0090</th>\n      <th>icd_code_01300</th>\n      <th>icd_code_01896</th>\n      <th>icd_code_0340</th>\n      <th>icd_code_035</th>\n      <th>icd_code_0380</th>\n      <th>icd_code_03811</th>\n      <th>icd_code_03812</th>\n      <th>...</th>\n      <th>icd_code_V5811</th>\n      <th>icd_code_V5812</th>\n      <th>icd_code_V5883</th>\n      <th>icd_code_V600</th>\n      <th>icd_code_V618</th>\n      <th>icd_code_V6284</th>\n      <th>icd_code_V714</th>\n      <th>icd_code_V7189</th>\n      <th>icd_code_V7281</th>\n      <th>icd_code_V7651</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4570</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4571</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4572</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4573</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4574</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4575 rows × 1203 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"icd_codes = []\nfor i in y.columns:\n    icd_codes.append(i[9:])\nicd_codes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"icd_df = pd.read_csv('/kaggle/input/icd-code-map/d_icd_diagnoses.csv')\nicd_df = icd_df[icd_df['icd_version'] == 9]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:28:33.544527Z","iopub.execute_input":"2025-02-11T20:28:33.544805Z","iopub.status.idle":"2025-02-11T20:28:33.712052Z","shell.execute_reply.started":"2025-02-11T20:28:33.544784Z","shell.execute_reply":"2025-02-11T20:28:33.711398Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"icd_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:28:36.738480Z","iopub.execute_input":"2025-02-11T20:28:36.738754Z","iopub.status.idle":"2025-02-11T20:28:36.747954Z","shell.execute_reply.started":"2025-02-11T20:28:36.738734Z","shell.execute_reply":"2025-02-11T20:28:36.747160Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"       icd_code  icd_version  \\\n0          0010            9   \n1          0011            9   \n2          0019            9   \n3          0020            9   \n4          0021            9   \n...         ...          ...   \n102715    V9129            9   \n102802    V9190            9   \n102803    V9191            9   \n102804    V9192            9   \n102805    V9199            9   \n\n                                               long_title  \n0                          Cholera due to vibrio cholerae  \n1                   Cholera due to vibrio cholerae el tor  \n2                                    Cholera, unspecified  \n3                                           Typhoid fever  \n4                                     Paratyphoid fever A  \n...                                                   ...  \n102715  Quadruplet gestation, unable to determine numb...  \n102802  Other specified multiple gestation, unspecifie...  \n102803  Other specified multiple gestation, with two o...  \n102804  Other specified multiple gestation, with two o...  \n102805  Other specified multiple gestation, unable to ...  \n\n[14666 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>icd_code</th>\n      <th>icd_version</th>\n      <th>long_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0010</td>\n      <td>9</td>\n      <td>Cholera due to vibrio cholerae</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0011</td>\n      <td>9</td>\n      <td>Cholera due to vibrio cholerae el tor</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0019</td>\n      <td>9</td>\n      <td>Cholera, unspecified</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0020</td>\n      <td>9</td>\n      <td>Typhoid fever</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0021</td>\n      <td>9</td>\n      <td>Paratyphoid fever A</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102715</th>\n      <td>V9129</td>\n      <td>9</td>\n      <td>Quadruplet gestation, unable to determine numb...</td>\n    </tr>\n    <tr>\n      <th>102802</th>\n      <td>V9190</td>\n      <td>9</td>\n      <td>Other specified multiple gestation, unspecifie...</td>\n    </tr>\n    <tr>\n      <th>102803</th>\n      <td>V9191</td>\n      <td>9</td>\n      <td>Other specified multiple gestation, with two o...</td>\n    </tr>\n    <tr>\n      <th>102804</th>\n      <td>V9192</td>\n      <td>9</td>\n      <td>Other specified multiple gestation, with two o...</td>\n    </tr>\n    <tr>\n      <th>102805</th>\n      <td>V9199</td>\n      <td>9</td>\n      <td>Other specified multiple gestation, unable to ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>14666 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# # Get top-3 predictions (with probabilities)\ntop_k_indices, top_k_probs = ensemble_topk(X_scaled, top_k=3, weights=[0.4, 0.3, 0.3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:33:26.206309Z","iopub.execute_input":"2025-02-11T20:33:26.206601Z","iopub.status.idle":"2025-02-11T20:37:15.577072Z","shell.execute_reply.started":"2025-02-11T20:33:26.206580Z","shell.execute_reply":"2025-02-11T20:37:15.576415Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"top_k_probs[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:42:00.219685Z","iopub.execute_input":"2025-02-11T20:42:00.220001Z","iopub.status.idle":"2025-02-11T20:42:00.225217Z","shell.execute_reply.started":"2025-02-11T20:42:00.219974Z","shell.execute_reply":"2025-02-11T20:42:00.224583Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([[0.60780269, 0.00900517, 0.0059068 ],\n       [0.55496096, 0.02033082, 0.009049  ],\n       [0.35938857, 0.01381496, 0.00984106],\n       [0.55590244, 0.00946842, 0.00580056],\n       [0.37038364, 0.01350639, 0.01231651]])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"def format_ensemble_predictions(top_k_indices, y_labels, icd_df, top_k_probs):\n    \"\"\"\n    Given an array of top k prediction indices for each sample,\n    a list of y_labels mapping indices to ICD codes, and a dataframe containing\n    ICD code details (with columns \"icd_code\" and \"long_title\"), returns a list\n    of formatted strings describing the predictions in natural language.\n\n    Args:\n        top_k_indices (np.array): Array of shape (n_samples, top_k) with predicted class indices.\n        y_labels (list): List of ICD code strings, mapping model output indices to ICD codes.\n        icd_df (pd.DataFrame): DataFrame with columns \"icd_code\" and \"long_title\".\n        \n    Returns:\n        List[str]: A list of formatted strings, one for each sample.\n    \"\"\"\n    formatted_outputs = []\n    n_samples, top_k = top_k_indices.shape\n    \n    for i in range(10):\n        # Convert predicted indices to ICD codes.\n        predicted_codes = [y_labels[idx] for idx in top_k_indices[i]]\n        # Lookup the long_title for each ICD code.\n        titles = []\n        probs = []\n        ct = 0\n        for code in predicted_codes:\n            # Find the row with the matching ICD code.\n            match = icd_df[icd_df['icd_code'] == code]\n            if not match.empty:\n                title = match.iloc[0]['long_title']\n            else:\n                title = \"Unknown condition\"\n            titles.append(title)\n            probs.append(top_k_probs[i][ct])\n            ct+=1\n        \n        # Format a natural language string.\n        formatted_str = f\"Sample {i+1}: The top {top_k} predicted diagnoses are:\\n\"\n        for rank, (code, title) in enumerate(zip(predicted_codes, titles), start=1):\n            prob_this = float(\"{:.2f}\".format(probs[rank-1]*100))\n            formatted_str += f\"  {rank}. {title} (ICD Code: {code}) with Probability : {prob_this}%\\n\"\n        \n        formatted_outputs.append(formatted_str)\n        \n    return formatted_outputs\n\n# Example usage:\n# Assume top_k_indices is obtained from ensemble_topk, y_labels is your list of ICD codes,\n# and icd_df is your DataFrame.\nformatted_results = format_ensemble_predictions(top_k_indices, icd_codes, icd_df, top_k_probs)\n\n# Print the formatted output for each sample.\nfor result in formatted_results:\n    print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T20:48:06.341436Z","iopub.execute_input":"2025-02-11T20:48:06.341755Z","iopub.status.idle":"2025-02-11T20:48:06.397801Z","shell.execute_reply.started":"2025-02-11T20:48:06.341728Z","shell.execute_reply":"2025-02-11T20:48:06.397086Z"}},"outputs":[{"name":"stdout","text":"Sample 1: The top 3 predicted diagnoses are:\n  1. Portal hypertension (ICD Code: 5723) with Probability : 60.78%\n  2. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 0.9%\n  3. Atrial fibrillation (ICD Code: 42731) with Probability : 0.59%\n\nSample 2: The top 3 predicted diagnoses are:\n  1. Unspecified viral hepatitis C with hepatic coma (ICD Code: 07071) with Probability : 55.5%\n  2. Hyposmolality and/or hyponatremia (ICD Code: 2761) with Probability : 2.03%\n  3. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 0.9%\n\nSample 3: The top 3 predicted diagnoses are:\n  1. Chronic hepatitis C without mention of hepatic coma (ICD Code: 07054) with Probability : 35.94%\n  2. Closed fracture of four ribs (ICD Code: 80704) with Probability : 1.38%\n  3. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 0.98%\n\nSample 4: The top 3 predicted diagnoses are:\n  1. Other iatrogenic hypotension (ICD Code: 45829) with Probability : 55.59%\n  2. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 0.95%\n  3. Acute pancreatitis (ICD Code: 5770) with Probability : 0.58%\n\nSample 5: The top 3 predicted diagnoses are:\n  1. Contusion of abdominal wall (ICD Code: 9222) with Probability : 37.04%\n  2. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 1.35%\n  3. Acute salpingitis and oophoritis (ICD Code: 6140) with Probability : 1.23%\n\nSample 6: The top 3 predicted diagnoses are:\n  1. Malignant neoplasm of kidney, except pelvis (ICD Code: 1890) with Probability : 61.61%\n  2. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 1.08%\n  3. Other chest pain (ICD Code: 78659) with Probability : 0.55%\n\nSample 7: The top 3 predicted diagnoses are:\n  1. First-degree perineal laceration, delivered, with or without mention of antepartum condition (ICD Code: 66401) with Probability : 59.67%\n  2. Early onset of delivery, delivered, with or without mention of antepartum condition (ICD Code: 64421) with Probability : 1.47%\n  3. Previous cesarean delivery, delivered, with or without mention of antepartum condition (ICD Code: 65421) with Probability : 1.33%\n\nSample 8: The top 3 predicted diagnoses are:\n  1. Closed fracture of nasal bones (ICD Code: 8020) with Probability : 35.38%\n  2. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 1.19%\n  3. Acute kidney failure, unspecified (ICD Code: 5849) with Probability : 0.85%\n\nSample 9: The top 3 predicted diagnoses are:\n  1. Alcoholic cirrhosis of liver (ICD Code: 5712) with Probability : 62.19%\n  2. Acute pancreatitis (ICD Code: 5770) with Probability : 1.17%\n  3. Coronary atherosclerosis of native coronary artery (ICD Code: 41401) with Probability : 0.82%\n\nSample 10: The top 3 predicted diagnoses are:\n  1. Acute alcoholic hepatitis (ICD Code: 5711) with Probability : 35.86%\n  2. Cellulitis and abscess of trunk (ICD Code: 6822) with Probability : 1.8%\n  3. Acute pancreatitis (ICD Code: 5770) with Probability : 1.38%\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}